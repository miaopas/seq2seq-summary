{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence modelling notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.initialize import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=width>\n",
    "\n",
    "### 1. Sequence to sequence modelling tasks\n",
    "\n",
    "Sequence to sequence modelling tasks are machine learning tasks where both the inputs and output are sequences.\n",
    "\n",
    "Some examples:\n",
    "<p align=\"left\">\n",
    "<img src=\"resources/assets/seq_to_seq_applications.png\" alt=\"drawing\" width=\"800\" >\n",
    "</p>\n",
    "\n",
    "Let's first see some examples of sequence to sequence modelling problems.\n",
    "</div>\n",
    "\n",
    "<div class=width>\n",
    "\n",
    "### Examples of sequence to sequence (seq2seq) tasks\n",
    "Let's look at some concrete seq2seq tasks for illustration. \n",
    "\n",
    "#### 1. Shift a sequence\n",
    "This is a toy example where we just shift a sequence to the right, and pad zeros at left. For example we want shift a input by three steps,\n",
    "$$\\begin{align*}\n",
    "\\text{Input:} & \\, 5,8,9,0,1,2,5,6 \\\\\n",
    "\\text{Output:} &\\,  0,0,0,5,8,9,0,1\n",
    "\\end{align*}$$\n",
    "Shifting inputs by $k $ units is actually a linear relation which equivalent to a convolution of the input with a delta impulse,\n",
    "$$\\begin{align*}\n",
    "y(t) = \\sum_s \\delta(s-k)x(t-s)\n",
    "\\end{align*}$$\n",
    "where \n",
    "$$\n",
    "\\delta(s-k) = \\begin{cases}\n",
    "                    1 & s = k \\\\\n",
    "                    0 & \\text{else}\n",
    "                \\end{cases}.\n",
    "$$\n",
    "\n",
    "This relationship is completely determined by the parameter $k$. If can also be considered as the memory of this relationship because we have $y(t) = y(t-k)$. thus when $k$ is large $y(t)$ will depend on a input far from it.\n",
    " \n",
    "Theoretically RNN does not perform well on this task while CNN have very good performance. \n",
    ">(See our paper [Approximation Theory of Convolutional Architectures for Time Series Modelling](https://proceedings.mlr.press/v139/jiang21d.html))\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "div.width {\n",
    "\n",
    "    margin:auto;\n",
    "    max-width: 1000px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ab050069574597adeef0178daec3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HBox(children=(HTML(value='&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'), Button(description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ShiftPlotter(k=25).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=width>\n",
    "\n",
    "#### 2. Convolution of a sequence\n",
    "Convolution is one of the most basic operation which can be considered as a sequence to sequence task.\n",
    "Suppose $\\bm\\rho$ is a convolution filter, then the convolution of the input $\\bm x$ with the filter is given by\n",
    "$$\\begin{align*}\n",
    "y(t) = \\bm\\rho \\ast\\bm x =\\sum_s \\rho(s)x(t-s).\n",
    "\\end{align*}$$\n",
    "In this case the filter $\\bm \\rho$ determine the relationship.\n",
    "\n",
    "<div>\n",
    "\n",
    "<style>\n",
    "div.width {\n",
    "\n",
    "    margin:auto;\n",
    "    max-width: 1000px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35f2b4fd78c456fb8fcede6e9d58854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HBox(children=(Text(value='0.002, 0.022, 0.097, 0.159, 0.097, 0.022, 0.002', des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConvoPlotter().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=width>\n",
    "\n",
    "#### 3. Lorentz96 System\n",
    "\n",
    "Now let's look at a more complicate example where the input ouput relationship is determined by an nonlinear dynamic system.\n",
    "  \n",
    "  \n",
    "The system have $K$ inputs $\\{x_k\\}$, $K$ outputs $\\{y_k\\}$ and $JK$ hidden variables $\\{z_{j,k}\\}$ with $k = 1, 2, \\dots, K$ and $j = 1, 2, \\dots, J$. The parameters $K,J$ control the number of variables in the system, and can be viewed as a complexity measure.\n",
    "The system satisfies the following dynamics\n",
    "\\begin{align*}\n",
    "    \\frac{dy_k}{dt} & = -y_{k-1}(y_{k-2}-y_{k+1})-y_k + {\\color{green} x_k}  - \\frac{1}{J}\\sum_{j=1}^J z_{j,k},  \\\\\n",
    "    \\frac{dz_{j,k}}{dt} & = -z_{j+1,k}(z_{j+2,k}-z_{j-1,k})-z_{j,k} + y_k.\n",
    "\\end{align*}\n",
    "\n",
    "Thus, given a set of input $\\{x_k\\}$, the systems determins a set of outputs $\\{y_k\\}$.\n",
    "The following plot shows an example with $K=1$, where we have one curve as input, and the system gives an output curve.\n",
    "<div>\n",
    "\n",
    "<style>\n",
    "div.width {\n",
    "\n",
    "    margin:auto;\n",
    "    max-width: 1000px;\n",
    "}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b455df54134387a0f1404a5d60551b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HBox(children=(HTML(value='&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'), Button(description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LorentzPlotter().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=width>\n",
    "\n",
    "#### 4. Text Generation\n",
    "\n",
    "This is a real life example for sequence prediction task. Given a begining of a sentence the model will try to write the remaining part. We can generate long paragraphs of articles using this. To generate nice and meaningful text we may need a very model, here we only use a small model for demostration.\n",
    "\n",
    "There are two types of models:\n",
    "- Character : Character level model, the model will generate single character each step.\n",
    "- Word : Word level model, the model generate a word each step.\n",
    "\n",
    "<div>\n",
    "\n",
    "<style>\n",
    "div.width {\n",
    "\n",
    "    margin:auto;\n",
    "    max-width: 1000px;\n",
    "}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d51b8b49aae43189a664d91406b39bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value=' <font size=\"+0.4\">Model Type: </font>'), ToggleButtons(options=('Ch…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TextGenerator().plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=width>\n",
    "\n",
    "### 2. Baisc Architectures for seq2seq modelling\n",
    "\n",
    "Next let's look at some basic architectures for seq2seq modelling, we will began with recurrent neural work (RNN), which is one of the most simple architectures.\n",
    "</div>\n",
    "\n",
    "<div class=width>\n",
    "\n",
    "#### 1.Recurrent neural networks (RNN)\n",
    "\n",
    "Recurrent neural network is the most basic sequence to sequence model. The dynamic can be written as \n",
    "$$\\begin{align*}\n",
    "h_{t+1} &= \\sigma(Wh_{t} + Ux_{t} + b)\\\\\n",
    "o_{t+1} &= c^\\top h_t.\n",
    "\\end{align*}$$ \n",
    "Where $h$ is called the hidden state. Note that this architecture is causal such that the output $o_t$ at time $t$ only depends on inputs up to $t$. \n",
    " \n",
    "<p align=\"center\">\n",
    "<img src=\"resources/assets/rnn.png\" alt=\"drawing\" width=\"500\" >\n",
    "</p>\n",
    "\n",
    "Based on the structure above we can have input output pairs having same length, which is typical supervised learning tasks. We can also feed the output $o_t$ as the input $x_{t+1}$, which forms an autoregressive stucture and are usually applied to time series prediciton  or sequence generation. \n",
    "\n",
    "In the following demo implementation, the model takes a input with size **(batch size, input len, input dim)**, and output having size **(batch size, input len, output dim)**.\n",
    "\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "div.width {\n",
    "\n",
    "    margin:auto;\n",
    "    max-width: 1000px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Implementation\n",
    "class RNN(Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 hid_dim,\n",
    "                 activation=nn.Tanh()\n",
    "                ):\n",
    "        super().__init__()   \n",
    "        self.U = nn.Linear(input_dim, hid_dim)\n",
    "        self.W = nn.Linear(hid_dim,hid_dim)\n",
    "        self.c = nn.Linear(hid_dim, output_dim)\n",
    "        self.hid_dim = hid_dim\n",
    "    def forward(self, x, initial_hidden=None):\n",
    "\n",
    "        #src = [batch size, input len, input dim]\n",
    "        length = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        hidden = []\n",
    "        # Initial hidden state\n",
    "        if initial_hidden is None:\n",
    "            hidden.append(torch.zeros(batch_size, 1, self.hid_dim, dtype=x.dtype, device=x.device))\n",
    "        else:\n",
    "            hidden.append(initial_hidden)\n",
    "            \n",
    "        # recurrent relation\n",
    "        for i in range(length):\n",
    "            h_next = self.activation(self.W(hidden[i]) + self.U(x)[:,i:i+1,:])\n",
    "            hidden.append(h_next)\n",
    "\n",
    "        # Convert all hidden into a tensor\n",
    "        hidden = torch.cat(hidden[1:], dim=1)\n",
    "\n",
    "        # output mapping\n",
    "        out = self.c(hidden)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=width>\n",
    "\n",
    "Let's now test the model on the tasks we mentioned above. and plot the prediction against the output.\n",
    "You can find the saved model parameters inside folder: `resources/saved_models/lorentz`\n",
    "\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "div.width {\n",
    "\n",
    "    margin:auto;\n",
    "    max-width: 1000px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd498c1cb954462b092826fe0094bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HBox(children=(HTML(value='&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'), Button(description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LorentzEvaluation(RNNModel, path='resources/saved_models/lorentz/rnn_1_10_128.ckpt').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=width>\n",
    "\n",
    "A simple RNN model can nearly learn the basic pattern of this relationship, for better performance we may need a larger and better structured model.\n",
    "Let's also take a look the training curve.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/assets/rnn_lorentz_loss.png\" alt=\"drawing\" width=\"600\" >\n",
    "</p>\n",
    "\n",
    "Note that there is a plateauing region where the loss nearly stops to decay. This phenomenon have been analysed in the work\n",
    "[Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks](https://www.jmlr.org/papers/volume23/21-0368/21-0368.pdf).\n",
    "The main idea is that RNNs are hard to learn targets with long memory, there will be a plateauing in the training loss, and the length of plateauing region is exponential to the memory.\n",
    "\n",
    "Let's next look at the shift sequence example. We generate a 32 step sequence and move it to the right by 8 steps. Let's first look at the tain loss.\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/assets/rnn_shiftseq_loss.png\" alt=\"drawing\" width=\"600\" >\n",
    "</p>\n",
    "The plateauing also occurs here. \n",
    "\n",
    "Next let's look at how the model makes the prediction, first click the `Refresh` button to randomly pick some samples.\n",
    "You may have note that, the prediction from the model seems not following the output patter well, instead, it tries to give a output which is near zero.\n",
    "The training data and the loss function are the main reasons lead to this kind of behaviour. The mean of all the training sequence actually is the constant zero sequence, and the loss we used is MSE loss. Thus when the model cannot properly learn the target, provide a constant zero output can make the loss small as the loss is taking average on all the inputs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "div.width {\n",
    "\n",
    "    margin:auto;\n",
    "    max-width: 1000px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bc9527614e4c26ada432f9780f7eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HBox(children=(HTML(value='&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'), Button(description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ShiftEvaluation(RNNModel, 'resources/saved_models/shift/rnn_8_32.ckpt').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=width>\n",
    "\n",
    "#### 2.Convolutional neural networks (CNN)\n",
    "\n",
    "CNNs are widely applied on computer vision related tasks, however a 1D convolution can also be applied to sequence related tasks since convolution is naturally a seq2seq mapping. To apply CNN on sequential data, people often use a dialted convolutions structure as shown in the picture below. By using small filters with dilation rate increasing exponentially, the model can finally achieve a very large filter with few parameters. For example in the following picutre, the convolutin filter achieve a receptive field of size 16 with only 8 parameters. This give arises a low rank structure, which have been disscussed in \n",
    "[Approximation Theory of Convolutional Architectures for Time Series Modelling](http://proceedings.mlr.press/v139/jiang21d.html).\n",
    " \n",
    "<p align=\"center\">\n",
    "<img src=\"resources/assets/cnn.png\" alt=\"drawing\" width=\"500\" >\n",
    "</p>\n",
    "\n",
    "In the paper we discussed the difference between CNN and RNN structure on seq2seq modelling problems. The approximation capability of CNN is better then RNN for targets which is not smooth or have long memory. Through experiments we can also see CNN indeed perform better on certain tasks. \n",
    "\n",
    "Next let's look at the shift sequence exampe and lorentz system example using convolutional structures.\n",
    "For the Lorentz system task, we can see that CNN performs much better then RNN as it captures the pattern very well. It also have a smoother training curve.\n",
    "\n",
    "The CNN model have about 40K parameters while the RNN model have around 791K parameters. Even though the RNN have much more parameters then a CNN, the performance may not be better.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/assets/tcn_lorentz_loss.png\" alt=\"drawing\" width=\"600\" >\n",
    "</p>\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<style>\n",
    "div.width {\n",
    "\n",
    "    margin:auto;\n",
    "    max-width: 1000px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64382eace06d4167b8f20747206c6c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HBox(children=(HTML(value='&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'), Button(description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LorentzEvaluation(TCNModel, path='resources/saved_models/lorentz/tcn_1_10_128.ckpt').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=width>\n",
    "\n",
    "For the shift seqeunce example we can hardly tell the differnece from the plot. Shifting a sequence is actually a convolution operation thus the CNN is able to represent it exactly. The training curve is shown in the following image.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"resources/assets/tcn_shift_loss.png\" alt=\"drawing\" width=\"600\" >\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "div.width {\n",
    "\n",
    "    margin:auto;\n",
    "    max-width: 1000px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e193ae2d11c949c9b9846d39f2cd8fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HBox(children=(HTML(value='&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'), Button(description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ShiftEvaluation(TCNModel, 'resources/saved_models/shift/tcn_32_128.ckpt').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c06b2f2ee114d58810f26bc6dad5247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HBox(children=(HTML(value='&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'), Button(description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ShiftEvaluation(TransformerModel, 'resources/saved_models/shift/transformer_32_128.ckpt').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbb2922baf64bccba959d58f4324ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HBox(children=(HTML(value='&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'), Button(description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LorentzEvaluation(TransformerModel, 'resources/saved_models/lorentz/transformer_1_10_128.ckpt').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ce0e8490cf730150a424e5807f7fde0b91ecc24faf3dcab8fe4b13f6f1ae3a5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
