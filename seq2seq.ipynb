{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence modelling notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from initialize import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdaaaecc1c74ab29a6a15724e60763d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='0.002, 0.022, 0.097, 0.159, 0.097, 0.022, 0.002', description='Filte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConvoPlotter().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to sequence modelling tasks\n",
    "\n",
    "Sequence to sequence modelling tasks are machine learning tasks where both the inputs and output are sequences.\n",
    "\n",
    "Some examples:\n",
    "<p align=\"left\">\n",
    "<img src=\"assets/seq_to_seq_applications.png\" alt=\"drawing\" width=\"800\" >\n",
    "</p>\n",
    "\n",
    "Let's first see some examples of sequence to sequence modelling problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of sequence to sequence (seq2seq) tasks\n",
    "Let's look at some concrete seq2seq tasks for illustration. \n",
    "\n",
    "#### 1. Shift a sequence\n",
    "This is a toy example where we just shift a sequence to the right, and pad zeros at left. For example we want shift a input by three steps,\n",
    "$$\\begin{align*}\n",
    "\\text{Input:} & \\, 5,8,9,0,1,2,5,6 \\\\\n",
    "\\text{Output:} &\\,  0,0,0,5,8,9,0,1\n",
    "\\end{align*}$$\n",
    "Shifting inputs by $k $ units is actually a linear relation which equivalent to a convolution of the input with a delta impulse,\n",
    "$$\\begin{align*}\n",
    "y(t) = \\sum_s \\delta(s-k)x(t-s)\n",
    "\\end{align*}$$\n",
    "where \n",
    "$$\n",
    "\\delta(s-k) = \\begin{cases}\n",
    "                    1 & s = k \\\\\n",
    "                    0 & \\text{else}\n",
    "                \\end{cases}.\n",
    "$$\n",
    "\n",
    "Theoretically RNN does not perform well on this task while CNN have very good performance. (See our paper [Approximation Theory of Convolutional Architectures for Time Series Modelling](https://proceedings.mlr.press/v139/jiang21d.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb98460cee0e4c8b870bfc5c64de470d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HBox(children=(Button(description='Refresh', style=ButtonStyle()),), layout=Layo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ShiftPlotter().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e0365242f24f45862c10e2ea21a844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HBox(children=(Button(description='Refresh', style=ButtonStyle()),), layout=Layo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LorentzPlotter().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural networks (RNN)\n",
    "\n",
    "Recurrent neural network is the most basic sequence to sequence model. The dynamic can be written as \n",
    "$$\\begin{align*}\n",
    "h_{t+1} &= \\sigma(Wh_{t} + Ux_{t} + b)\\\\\n",
    "o_{t+1} &= c^\\top h_t.\n",
    "\\end{align*}$$ \n",
    "Where $h$ is called the hidden state. Note that this architecture is causal such that the output $o_t$ at time $t$ only depends on inputs up to $t$. \n",
    " \n",
    "<p align=\"center\">\n",
    "<img src=\"assets/rnn.png\" alt=\"drawing\" width=\"500\" >\n",
    "</p>\n",
    "\n",
    "Based on the structure above we can have input output pairs having same length, which is typical supervised learning tasks. We can also feed the output $o_t$ as the input $x_{t+1}$, which forms an autoregressive stucture and are usually applied to time series prediciton  or sequence generation. \n",
    "\n",
    "In the following implementation, the model takes a input with size **(batch size, input len, input dim)**, and output having size **(batch size, input len, output dim)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo experiments\n",
    "\n",
    "Next we show some concrete examples on sequence to sequence tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCN(Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 hid_dim,\n",
    "                 kernel_size,\n",
    "                 num_layers,\n",
    "                 activation='linear'\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.ModuleList([nn.Conv1d(hid_dim,hid_dim,kernel_size,padding=(kernel_size-1)*(kernel_size**i),dilation=kernel_size**i,bias=False) for i in range(num_layers)])\n",
    "        self.input_ff = nn.Linear(input_dim, hid_dim)\n",
    "        self.output_ff = nn.Linear(hid_dim, output_dim)\n",
    "        if activation == 'linear':\n",
    "            self.activation = nn.Identity()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Uknow actication type\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        #src = [batch size, input len, input dim]\n",
    "        length = x.shape[1]\n",
    "        x = self.input_ff(x)\n",
    "\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        for layer in self.conv_layers:\n",
    "            x = x + layer(x)[:,:,:length]\n",
    "       \n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        y = self.activation(self.output_ff(x))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN encoder-decoder\n",
    "\n",
    "Here we introduce the encoder-decoder structure, where we first encodes the input into  the context vector and then decodes the context vector into the output. Suppose both encoder and decoder are RNNs then we can write this relation as \n",
    "$$\n",
    "\\begin{align*}\n",
    "h_s &= \\sigma_E( W_Eh_{s-1}+U_Ex_s+ b_E), \n",
    "    \\hspace{5mm} v = h_\\tau,\\\\\n",
    "     g_t &= \n",
    "     %\\hl{\n",
    "     \\sigma_D( W_Dg_{t-1}+ b_D),\n",
    "     %}\n",
    "    \\hspace{16mm} g_0=v, \\\\\n",
    "    o_t&= \n",
    "    %\\hl{\n",
    "    W_O g_t+b_O.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"assets/encdec.png\" alt=\"drawing\" width=\"500\" >\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDec(Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 hid_dim,\n",
    "                 output_len,\n",
    "                 activation='linear'\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder = RNN(input_dim,output_dim, hid_dim, activation, return_hidden=True)\n",
    "        self.decoder = RNN(input_dim,output_dim, hid_dim, activation)\n",
    "        self.out_len = output_len\n",
    "    def forward(self, x):\n",
    "        _, context = self.encoder(x)\n",
    "        context = context[:,-2:-1,:]\n",
    "        batch_size = x.shape[0]\n",
    "        decoder_input_pad = torch.zeros(batch_size,self.out_len,x.shape[-1], dtype=x.dtype, device=x.device)\n",
    "\n",
    "        y = self.decoder(decoder_input_pad, context)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = f'Shift'\n",
    "\n",
    "train_size = 3000\n",
    "test_size = 500\n",
    "\n",
    "train_dataset = Dataset(*Shift({'path_len':32,'shift': 20}).generate(data_num=train_size), dtype=DTYPE, device=device)\n",
    "test_dataset = Dataset(*Shift({'path_len':32,'shift': 20}).generate(data_num=test_size), dtype=DTYPE, device=device)\n",
    "\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=128,drop_last=True)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=128,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,153 trainable parameters\n",
      "Best Valid Loss: 1.95e-05\n"
     ]
    }
   ],
   "source": [
    "experiment_name = f'{data_name}_rnn'\n",
    "rnn = RNN(input_dim=1, output_dim=1, hid_dim=32).double().to(device)\n",
    "rnn.load_state_dict(torch.load(f\"saved_model/{experiment_name}/best_valid.pt\"))\n",
    "rnn.count_parameters()\n",
    "# train_model(name=experiment_name,model=rnn,train_data=train_data, test_data=test_data)\n",
    "print(f'Best Valid Loss: {np.mean(inference(rnn, test_data)):.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 14 trainable parameters\n",
      "Best Valid Loss: 2.28e-32\n"
     ]
    }
   ],
   "source": [
    "experiment_name = f'{data_name}_cnn'\n",
    "cnn = DCN(input_dim=1, output_dim=1, hid_dim=1, kernel_size=2, num_layers=5).double().to(device)\n",
    "cnn.load_state_dict(torch.load(f\"saved_model/{experiment_name}/best_valid.pt\"))\n",
    "cnn.count_parameters()\n",
    "# train_model(name=experiment_name,model=cnn,train_data=train_data, test_data=test_data)\n",
    "print(f'Best Valid Loss: {np.mean(inference(cnn, test_data)):.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.135e-02 \t Val. Loss: 3.129e-02 \t Best Loss: 3.135e-02 \t Current lr: 1.000e-08: | 1000/1000 [06:30<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Valid Loss: 3.13e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_name = f'{data_name}_encdec'\n",
    "encdec = EncDec(1, 1, hid_dim=64, output_len=32).double().to(device)\n",
    "# encdec.load_state_dict(torch.load(f\"saved_model/{experiment_name}/best_valid.pt\"))\n",
    "train_model(name=experiment_name,model=encdec,train_data=train_data, test_data=test_data)\n",
    "print(f'Best Valid Loss: {np.mean(inference(encdec, test_data)):.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(X,y , dtype=DTYPE, device='cuda')\n",
    "\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=128,drop_last=True)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=128,drop_last=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4461580811ecaa8d2658fc25e124c6bfb2f5eb0b671554e73c2a686519281506"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
