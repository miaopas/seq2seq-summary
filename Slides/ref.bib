Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Palis1974,
author = {Palis, J},
issn = {0002-9904},
journal = {Bulletin of the American Mathematical Society},
number = {3},
pages = {503--505},
title = {{Vector fields generate few diffeomorphisms}},
volume = {80},
year = {1974}
}
@article{Li2018,
author = {Li, Huan and Yang, Yibo and Chen, Dongmin and Lin, Zhouchen},
journal = {arXiv preprint arXiv:1810.01638},
title = {{Optimization algorithm inspired deep neural network structure design}},
year = {2018}
}
@article{Du2018,
author = {Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
journal = {arXiv preprint arXiv:1810.02054},
title = {{Gradient descent provably optimizes over-parametrized neural networks}},
year = {2018}
}
@article{Effland2019,
author = {Effland, Alexander and Kobler, Erich and Kunisch, Karl and Pock, Thomas},
journal = {arXiv preprint arXiv:1907.08488},
title = {{An Optimal Control Approach to Early Stopping Variational Methods for Image Restoration}},
year = {2019}
}
@article{Bartlett2018,
abstract = {We show that any smooth bi-Lipschitz {\$}h{\$} can be represented exactly as a composition {\$}h{\_}m \backslashcirc ... \backslashcirc h{\_}1{\$} of functions {\$}h{\_}1,...,h{\_}m{\$} that are close to the identity in the sense that each {\$}\backslashleft(h{\_}i-\backslashmathrm{\{}Id{\}}\backslashright){\$} is Lipschitz, and the Lipschitz constant decreases inversely with the number {\$}m{\$} of functions composed. This implies that {\$}h{\$} can be represented to any accuracy by a deep residual network whose nonlinear layers compute functions with a small Lipschitz constant. Next, we consider nonlinear regression with a composition of near-identity nonlinear maps. We show that, regarding Fr$\backslash$'echet derivatives with respect to the {\$}h{\_}1,...,h{\_}m{\$}, any critical point of a quadratic criterion in this near-identity region must be a global minimizer. In contrast, if we consider derivatives with respect to parameters of a fixed-size residual network with sigmoid activation functions, we show that there are near-identity critical points that are suboptimal, even in the realizable case. Informally, this means that functional gradient methods for residual networks cannot get stuck at suboptimal critical points corresponding to near-identity layers, whereas parametric gradient methods for sigmoidal residual networks suffer from suboptimal critical points in the near-identity region.},
archivePrefix = {arXiv},
arxivId = {1804.05012},
author = {Bartlett, Peter L. and Evans, Steven N. and Long, Philip M.},
eprint = {1804.05012},
file = {:Users/qianxiao/Desktop/1804.05012.pdf:pdf},
keywords = {deep learning,optimization,residual networks},
pages = {1--13},
title = {{Representing smooth functions as compositions of near-identity functions with implications for deep network optimization}},
url = {http://arxiv.org/abs/1804.05012},
volume = {1},
year = {2018}
}
@article{rozonoer1959maximum,
author = {Rozonoer, Lev I},
journal = {Automation and Remote Control},
number = {10},
pages = {11},
title = {{The Maximum Principle of LS Pontryagin in Optimal-System Theory}},
volume = {20},
year = {1959}
}
@techreport{cardaliaguet2010notes,
author = {Cardaliaguet, Pierre},
institution = {Technical report},
title = {{Notes on mean field games}},
year = {2010}
}
@article{Zhang2016,
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
journal = {arXiv preprint arXiv:1611.03530},
title = {{Understanding deep learning requires rethinking generalization}},
year = {2016}
}
@article{halkin1966maximum,
author = {Halkin, Hubert},
journal = {SIAM Journal on control},
number = {1},
pages = {90--111},
publisher = {SIAM},
title = {{A maximum principle of the {\{}P{\}}ontryagin type for systems described by nonlinear difference equations}},
volume = {4},
year = {1966}
}
@article{Allen-Zhu2018,
author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
journal = {arXiv preprint arXiv:1811.03962},
title = {{A convergence theory for deep learning via over-parameterization}},
year = {2018}
}
@article{Ruthotto2018,
author = {Ruthotto, Lars and Haber, Eldad},
journal = {arXiv preprint arXiv:1804.04272},
title = {{Deep neural networks motivated by partial differential equations}},
year = {2018}
}
@article{Wu2017,
author = {Wu, Lei and Zhu, Zhanxing},
journal = {arXiv preprint arXiv:1706.10239},
title = {{Towards understanding generalization of deep learning: Perspective of loss landscapes}},
year = {2017}
}
@book{Agrachev2004,
abstract = {This book presents some facts and methods of Mathematical Control Theory treated from the geometric viewpoint. It is devoted to finite-dimensional deterministic control systems governed by smooth ordinary differential equations. The problems of controllability, state and feedback equivalence, and optimal control are studied. Some of the topics treated by the authors are covered in monographic or textbook literature for the first time while others are presented in a more general and flexible setting than elsewhere. Although being fundamentally written for mathematicians, the authors make an attempt to reach both the practitioner and the theoretician by blending the theory with applications. They maintain a good balance between the mathematical integrity of the text and the conceptual simplicity that might be required by engineers. It can be used as a text for graduate courses and will become most valuable as a reference work for graduate students and researchers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Agrachev, Andrei A. and Sachkov, Yuri L.},
booktitle = {Automatica},
doi = {10.1007/978-3-662-06404-7},
eprint = {arXiv:1011.1669v3},
file = {:Users/qianxiao/OneDrive - National University of Singapore/Research/Papers/controllability/notes.pdf:pdf},
isbn = {978-3-642-05907-0},
issn = {00051098},
number = {4},
pages = {695--696},
pmid = {25246403},
title = {{Control Theory from the Geometric Viewpoint}},
volume = {87},
year = {2004}
}
@book{pontryagin1987mathematical,
author = {Pontryagin, Lev S},
publisher = {CRC Press},
title = {{Mathematical theory of optimal processes}},
year = {1987}
}
@article{Sonoda2019,
author = {Sonoda, Sho and Murata, Noboru},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
number = {1},
pages = {31--82},
publisher = {JMLR. org},
title = {{Transport analysis of infinitely deep neural network}},
volume = {20},
year = {2019}
}
@article{Ma,
author = {Ma, Linjian and Sun, Yufan and Ding, Chiyu},
title = {{An empirical study of neural ordinal differential equations}}
}
@article{Daubechies2019,
author = {Daubechies, Ingrid and DeVore, Ronald and Foucart, Simon and Hanin, Boris and Petrova, Guergana},
journal = {arXiv preprint arXiv:1905.02199},
title = {{Nonlinear approximation and (deep) relu networks}},
year = {2019}
}
@article{Zhang2019,
author = {Zhang, Dinghuai and Zhang, Tianyuan and Lu, Yiping and Zhu, Zhanxing and Dong, Bin},
journal = {arXiv preprint arXiv:1905.00877},
title = {{You only propagate once: Painless adversarial training using maximal principle}},
year = {2019}
}
@article{Li2019,
author = {Li, Qianxiao and Tai, Cheng and Shen, Zuowei},
journal = {In Preparation},
title = {{Deep Approximation via Deep Learning}},
year = {2019}
}
@techreport{boltyanskii1960theory,
author = {Boltyanskii, Vladimir G and Gamkrelidze, Revaz V and Pontryagin, Lev S},
institution = {TRW Space Tochnology Labs, Los Angeles, California},
title = {{The theory of optimal processes. {\{}I{\}}. {\{}T{\}}he maximum principle}},
year = {1960}
}
@article{holtzman1966convexity,
author = {Holtzman, J},
journal = {IEEE Transactions on Automatic Control},
number = {1},
pages = {30--35},
publisher = {IEEE},
title = {{Convexity and the maximum principle for discrete systems}},
volume = {11},
year = {1966}
}
@article{Wang2018,
author = {Wang, Bao and Yuan, Binjie and Shi, Zuoqiang and Osher, Stanley J},
journal = {arXiv preprint arXiv:1811.10745},
title = {{Enresnet: Resnet ensemble via the feynman-kac formalism}},
year = {2018}
}
@inproceedings{Tao2018,
author = {Tao, Yunzhe and Sun, Qi and Du, Qiang and Liu, Wei},
booktitle = {Advances in Neural Information Processing Systems},
pages = {496--506},
title = {{Nonlocal neural networks, nonlocal diffusion and nonlocal modeling}},
year = {2018}
}
@inproceedings{li2017stochastic,
author = {Li, Qianxiao and Tai, Cheng and E, Weinan},
booktitle = {International Conference on Machine Learning (ICML 2017)},
title = {{Stochastic modified equations and adaptive stochastic gradient algorithms}},
year = {2017}
}



@book{bliss1946lectures,
author = {Bliss, Gilbert A},
publisher = {Chicago Univ. Press},
title = {{Lectures on the Calculus of Variations}},
year = {1946}
}
@inproceedings{Veit2016,
author = {Veit, Andreas and Wilber, Michael J and Belongie, Serge},
booktitle = {Advances in neural information processing systems},
pages = {550--558},
title = {{Residual networks behave like ensembles of relatively shallow networks}},
year = {2016}
}
@article{Zhang2019a,
author = {Zhang, Jingfeng and Han, Bo and Wynter, Laura and Low, Kian Hsiang and Kankanhalli, Mohan},
journal = {arXiv preprint arXiv:1902.10887},
title = {{Towards robust resnet: A small step but a giant leap}},
year = {2019}
}
@article{Parpas2019,
author = {Parpas, Panos and Muir, Corey},
journal = {arXiv preprint arXiv:1902.02542},
title = {{Predict globally, correct locally: Parallel-in-time optimal control of neural networks}},
year = {2019}
}
@article{Bellman1966,
author = {Bellman, Richard},
issn = {0036-8075},
journal = {Science},
number = {3731},
pages = {34--37},
publisher = {American Association for the Advancement of Science},
title = {{Dynamic programming}},
volume = {153},
year = {1966}
}
@article{weinan2019mean,
author = {E, Weinan and Han, Jiequn and Li, Qianxiao},
issn = {2522-0144},
journal = {Research in the Mathematical Sciences},
number = {1},
pages = {10},
publisher = {Springer},
title = {{A mean-field optimal control formulation of deep learning}},
volume = {6},
year = {2019}
}
@article{Chukwu1991,
abstract = {In abstract spaces, we consider certain constrained controllability and approximate controllability properties of a nonlinear system that can be deduced from various controllability properties of its associated linear system. Several examples involving partial differential operators and functional delay operators are given to illustrate the theory.},
author = {Chukwu, E. N. and Lenhart, S. M.},
doi = {10.1007/BF00940064},
file = {:Users/qianxiao/OneDrive - National University of Singapore/Research/Papers/controllability/Abstract{\_}controllability{\_}problem.pdf:pdf},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Constrained controls,approximate controllability,nonlinear systems},
number = {3},
pages = {437--462},
title = {{Controllability questions for nonlinear systems in abstract spaces}},
volume = {68},
year = {1991}
}
@article{Bao2019,
abstract = {It is known that fully connected neural networks with one hidden layer have the so called " universal approximation property " , yet such vanilla networks are seldom used in practice. Most recent progress in computer vision are powered by deep convolutional nets, but there is no approximation result on these architectures. In this paper, we fill the gap by showing that convolutional nets with global pooling also have the " universal approximation prop-erty " and characterizing its sample complexity. Moreover, we show that when the target function class has a certain compositional form, convolutional nets are far more advanta-geous compared with fully connected nets, in terms of number of parameters, number of samples and computational cost needed to achieve the desired accuracy.},
author = {Bao, Chenglong and Shen, Zuowei and Tai, Cheng and Wu, Lei and Xiang, Xueshuang and Li, Qianxiao and Shen, Zuowei and Tai, Cheng and Wu, Lei and Xiang, Xueshuang},
file = {:Users/qianxiao/Library/Application Support/Mendeley Desktop/Downloaded/Bao et al. - Unknown - Approximation and Scaling Analysis of Convolutional Neural Networks.pdf:pdf},
journal = {Under Review},
keywords = {,approximation,compositional functions,convolutional nets,scaling analysis},
pages = {1--26},
title = {{Approximation and Scaling Analysis of Convolutional Neural Networks}},
year = {2019}
}
@book{bressan2007introduction,
author = {Bressan, Alberto and Piccoli, Benedetto},
publisher = {American institute of mathematical sciences Springfield},
title = {{Introduction to the mathematical theory of control}},
volume = {2},
year = {2007}
}
@book{bryson1975applied,
author = {Bryson, Arthur Earl},
publisher = {CRC Press},
title = {{Applied optimal control: optimization, estimation and control}},
year = {1975}
}
@article{Zhang2018a,
author = {Zhang, Linfeng and Wang, Lei},
journal = {arXiv preprint arXiv:1809.10188},
title = {{Monge-Ampere Flow for Generative Modeling}},
year = {2018}
}
@article{holtzman1966discretional,
author = {Holtzman, Jack M and Halkin, Hubert},
journal = {SIAM Journal on Control},
number = {2},
pages = {263--275},
publisher = {SIAM},
title = {{Discretional convexity and the maximum principle for discrete systems}},
volume = {4},
year = {1966}
}
@techreport{Poggio2015,
author = {Poggio, Tomaso and Anselmi, Fabio and Rosasco, Lorenzo},
publisher = {Center for Brains, Minds and Machines (CBMM)},
title = {{I-theory on depth vs width: hierarchical function composition}},
year = {2015}
}
@inproceedings{Sun2016,
author = {Sun, Jian and Li, Huibin and Xu, Zongben},
booktitle = {Advances in neural information processing systems},
pages = {10--18},
title = {{Deep ADMM-Net for compressive sensing MRI}},
year = {2016}
}
@article{Zhu2018,
author = {Zhu, Mai and Chang, Bo and Fu, Chong},
journal = {arXiv preprint arXiv:1802.08831},
title = {{Convolutional neural networks combined with runge-kutta methods}},
year = {2018}
}
@article{robbins1951stochastic,
author = {Robbins, Herbert and Monro, Sutton},
journal = {The annals of mathematical statistics},
pages = {400--407},
publisher = {JSTOR},
title = {{A stochastic approximation method}},
year = {1951}
}
@inproceedings{Sonoda2017,
author = {Sonoda, Sho and Murata, Noboru},
booktitle = {ICML Workshop Principled Approaches to Deep Learning},
title = {{Double continuum limit of deep neural networks}},
year = {2017}
}
@inproceedings{pmlr-v80-li18b,
abstract = {Deep learning is formulated as a discrete-time optimal control problem. This allows one to characterize necessary conditions for optimality and develop training algorithms that do not rely on gradients with respect to the trainable parameters. In particular, we introduce the discrete-time method of successive approximations (MSA), which is based on the Pontryagin's maximum principle, for training neural networks. A rigorous error estimate for the discrete MSA is obtained, which sheds light on its dynamics and the means to stabilize the algorithm. The developed methods are applied to train, in a rather principled way, neural networks with weights that are constrained to take values in a discrete set. We obtain competitive performance and interestingly, very sparse weights in the case of ternary networks, which may be useful in model deployment in low-memory devices.},
author = {Li, Qianxiao and Hao, Shuji},
booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML 2018)},
pages = {2985--2994},
title = {{An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks}},
volume = {80},
year = {2018}
}
@article{Glass2012,
author = {Glass, Olivier},
doi = {10.1007/978-1-4614-1806-1_46},
file = {:Users/qianxiao/OneDrive - National University of Singapore/Research/Papers/controllability/IDC.pdf:pdf},
journal = {Mathematics of Complexity and Dynamical Systems},
pages = {755--770},
title = {{Infinite Dimensional Controllability}},
year = {2012}
}
@article{Ma2019,
author = {Ma, Chao and Wu, Lei},
journal = {arXiv preprint arXiv:1904.04326},
title = {{A comparative analysis of the optimization and generalization property of two-layer neural network and random feature models under gradient descent dynamics}},
year = {2019}
}
@inproceedings{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
isbn = {9781467388504},
issn = {10636919},
title = {{Deep residual learning for image recognition}},
year = {2016}
}
@article{Shen2019,
author = {Shen, Zuowei and Yang, Haizhao and Zhang, Shijun},
journal = {arXiv preprint arXiv:1902.10170},
title = {{Nonlinear approximation via compositions}},
year = {2019}
}
@article{holtzman1966maximum,
author = {Holtzman, J},
journal = {IEEE Transactions on Automatic Control},
number = {2},
pages = {273--274},
publisher = {IEEE},
title = {{On the maximum priciple for nonlinear discrete-time systems}},
volume = {11},
year = {1966}
}
@article{li2017maximum,
  title={Maximum principle based algorithms for deep learning},
  author={Li, Qianxiao and Chen, Long and Tai, Cheng and E, Weinan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={5998--6026},
  year={2017},
  publisher={JMLR. org}
}
@inproceedings{le1988theoretical,
author = {LeCun, Yann},
booktitle = {The Connectionist Models Summer School},
pages = {21--28},
title = {{A theoretical framework for back-propagation}},
volume = {1},
year = {1988}
}
@article{Chang2017,
author = {Chang, Bo and Meng, Lili and Haber, Eldad and Tung, Frederick and Begert, David},
journal = {arXiv preprint arXiv:1710.10348},
title = {{Multi-level residual networks from dynamical systems view}},
year = {2017}
}
@article{Brenier2003,
author = {Brenier, Yann and Gangbo, Wilfrid},
issn = {0944-2669},
journal = {Calculus of Variations and Partial Differential Equations},
number = {2},
pages = {147--164},
publisher = {Springer},
title = {{{\$} L{\^{}} p {\$} Approximation of maps by diffeomorphisms}},
volume = {16},
year = {2003}
}
@inproceedings{Chen2015,
author = {Chen, Yunjin and Yu, Wei and Pock, Thomas},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
pages = {5261--5269},
title = {{On learning optimized reaction diffusion processes for effective image restoration}},
year = {2015}
}
@article{Hermann1977,
abstract = {The properties of controllability, observability, and the theory of minimal realization for linear systems are well-understood and have been very useful in analyzing such systems. This paper deals with analogous questions for nonlinear systems.},
author = {Hermann, Robert and Krener, Arthur J.},
doi = {10.1109/TAC.1977.1101601},
file = {:Users/qianxiao/OneDrive - National University of Singapore/Research/Papers/controllability/IEEE Transactions on Automatic Control Volume 22 issue 5 1977 [doi 10.1109{\%}2FTAC.1977.1101601] Hermann, R.$\backslash$; Krener, A. -- Nonlinear controllability and observability.pdf:pdf},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
number = {5},
pages = {728--740},
title = {{Nonlinear Controllability and Observability}},
volume = {22},
year = {1977}
}
@article{Sun2018,
author = {Sun, Qi and Tao, Yunzhe and Du, Qiang},
journal = {arXiv preprint arXiv:1812.00174},
title = {{Stochastic training of residual networks: a differential equation viewpoint}},
year = {2018}
}
@article{Lu2019,
author = {Lu, Yiping and Li, Zhuohan and He, Di and Sun, Zhiqing and Dong, Bin and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
journal = {arXiv preprint arXiv:1906.02762},
title = {{Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View}},
year = {2019}
}
@article{Haber2017,
author = {Haber, Eldad and Ruthotto, Lars},
issn = {0266-5611},
journal = {Inverse Problems},
number = {1},
pages = {14004},
publisher = {IOP Publishing},
title = {{Stable architectures for deep neural networks}},
volume = {34},
year = {2017}
}
@inproceedings{courbariaux2015binaryconnect,
author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
booktitle = {Advances in Neural Information Processing Systems},
pages = {3123--3131},
title = {{Binaryconnect: Training deep neural networks with binary weights during propagations}},
year = {2015}
}
@article{Avelin2019,
author = {Avelin, Benny and Nystr{\"{o}}m, Kaj},
journal = {arXiv preprint arXiv:1906.12183},
title = {{Neural ODEs as the Deep Limit of ResNets with constant weights}},
year = {2019}
}
@article{aleksandrov1968accumulation,
author = {Aleksandrov, Vladimir V},
journal = {Vestnik MGU},
title = {{On the accumulation of perturbations in the linear systems with two coordinates}},
volume = {3},
year = {1968}
}
@inproceedings{Jia2019,
author = {Jia, Xixi and Liu, Sanyang and Feng, Xiangchu and Zhang, Lei},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {6054--6063},
title = {{FOCNet: A Fractional Optimal Control Network for Image Denoising}},
year = {2019}
}
@article{Zhou2019,
author = {Zhou, Ding-Xuan},
issn = {1063-5203},
journal = {Applied and Computational Harmonic Analysis},
publisher = {Elsevier},
title = {{Universality of deep convolutional neural networks}},
year = {2019}
}
@article{li2016ternary,
author = {Li, Fengfu and Zhang, Bo and Liu, Bin},
journal = {arXiv preprint arXiv:1605.04711},
title = {{Ternary weight networks}},
year = {2016}
}
@inproceedings{pmlr-v80-lu18d,
abstract = {Deep neural networks have become the state-of-the-art models in numerous machine learning tasks. However, general guidance to network architecture design is still missing. In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multi-step method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by applying the LM-architecture on ResNet and ResNeXt respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comparable numbers of trainable parameters. In particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress ({\textgreater}50{\%}) the original networks while maintaining a similar performance. This can be explained mathematically using the concept of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training process which helps to improve generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily apply stochastic training to the networks with the LM-architecture. As an example, we introduced stochastic depth to LM-ResNet and achieve significant improvement over the original LM-ResNet on CIFAR10.},
author = {Lu, Yiping and Zhong, Aoxiao and Li, Quanzheng and Dong, Bin},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
pages = {3276--3285},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations}},
volume = {80},
year = {2018}
}
@techreport{Poggio2015,
author = {Poggio, Tomaso and Anselmi, Fabio and Rosasco, Lorenzo},
publisher = {Center for Brains, Minds and Machines (CBMM)},
title = {{I-theory on depth vs width: hierarchical function composition}},
year = {2015}
}
@article{jackson1965discrete,
author = {Jackson, R and Horn, F},
journal = {International Journal of Control},
number = {4},
pages = {389--395},
publisher = {Taylor {\&} Francis},
title = {{On discrete analogues of Pontryagin's maximum principle}},
volume = {1},
year = {1965}
}
@article{Sun2019,
author = {Sun, Yifan and Zhang, Linan and Schaeffer, Hayden},
journal = {arXiv preprint arXiv:1908.03190},
title = {{NeuPDE: Neural Network Based Ordinary and Partial Differential Equations for Modeling Time-Dependent Data}},
year = {2019}
}
@article{li2015dynamics,
author = {Li, Qianxiao and Tai, Cheng and E, Weinan},
journal = {arXiv preprint arXiv:1511.06251v1},
title = {{Dynamics of stochastic gradient algorithms.}},
year = {2015}
}
@article{weinan2017proposal,
author = {E, Weinan},
issn = {2194-6701},
journal = {Communications in Mathematics and Statistics},
number = {1},
pages = {1--11},
publisher = {Springer},
title = {{A Proposal on Machine Learning via Dynamical Systems}},
volume = {5},
year = {2017}
}
@inproceedings{Lin2018,
author = {Lin, Hongzhou and Jegelka, Stefanie},
booktitle = {Advances in Neural Information Processing Systems},
pages = {6169--6178},
title = {{Resnet with one-neuron hidden layers is a universal approximator}},
year = {2018}
}
@article{Jastrzebski2017,
author = {Jastrzebski, Stanislaw and Arpit, Devansh and Ballas, Nicolas and Verma, Vikas and Che, Tong and Bengio, Yoshua},
journal = {arXiv preprint arXiv:1710.04773},
keywords = {li2019deepapprox},
mendeley-tags = {li2019deepapprox},
title = {{Residual connections encourage iterative inference}},
year = {2017}
}
@article{Tzen2019,
author = {Tzen, Belinda and Raginsky, Maxim},
journal = {arXiv preprint arXiv:1905.09883},
title = {{Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit}},
year = {2019}
}
@book{athans2013optimal,
author = {Athans, Michael and Falb, Peter L},
publisher = {Courier Corporation},
title = {{Optimal control: an introduction to the theory and its applications}},
year = {2013}
}
@article{Daubechies2019,
author = {Daubechies, Ingrid and DeVore, Ronald and Foucart, Simon and Hanin, Boris and Petrova, Guergana},
journal = {arXiv preprint arXiv:1905.02199},
title = {{Nonlinear approximation and (deep) relu networks}},
year = {2019}
}
@article{Zhang2018b,
author = {Zhang, Xiaoshuai and Lu, Yiping and Liu, Jiaying and Dong, Bin},
journal = {arXiv preprint arXiv:1805.07709},
title = {{Dynamically unfolding recurrent restorer: A moving endpoint control method for image restoration}},
year = {2018}
}
@inproceedings{Chen2018,
author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
booktitle = {Advances in neural information processing systems},
pages = {6571--6583},
title = {{Neural ordinary differential equations}},
year = {2018}
}
@article{Liu2019,
author = {Liu, Hailiang and Markowich, Peter},
journal = {arXiv preprint arXiv:1905.09076},
title = {{Selection dynamics for deep neural networks}},
year = {2019}
}
@inproceedings{Li2018b,
author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
booktitle = {Advances in Neural Information Processing Systems},
pages = {6389--6399},
title = {{Visualizing the loss landscape of neural nets}},
year = {2018}
}
@article{Thorpe2018,
author = {Thorpe, Matthew and van Gennip, Yves},
journal = {arXiv preprint arXiv:1810.11741},
title = {{Deep limits of residual neural networks}},
year = {2018}
}
@inproceedings{Chang2018,
author = {Chang, Bo and Meng, Lili and Haber, Eldad and Ruthotto, Lars and Begert, David and Holtham, Elliot},
booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
title = {{Reversible architectures for arbitrarily deep residual neural networks}},
year = {2018}
}
@article{krylov1962msa,
author = {Krylov, Ivan A and Chernousko, Felix L},
journal = {J. Comp. Mathem. and Mathematical Physics},
number = {6},
title = {{On the method of successive approximations for solution of optimal control problems}},
volume = {2},
year = {1962}
}
@article{butkovsky1963necessary,
author = {Butkovsky, Anatolii B},
journal = {Avtomat. i Telemekh},
number = {8},
pages = {1056--1064},
title = {{Necessary and sufficient optimality conditions for sampled-data control systems}},
volume = {24},
year = {1963}
}
@article{He2019,
author = {He, Juncai and Xu, Jinchao},
issn = {1674-7283},
journal = {Science China Mathematics},
pages = {1--24},
publisher = {Springer},
title = {{MgNet: A unified framework of multigrid and convolutional neural network}},
year = {2019}
}
@article{Balachandran2002,
abstract = {This paper presents a survey on research using fixed-point theorems and semigroup theory to study the controllability of nonlinear systems and functional integrodifferential systems in Banach spaces. Also discussed is the use of this technique in K-controllability and boundary controllability problems for nonlinear systems and integrodifferential systems in abstract spaces.},
author = {Balachandran, K. and Dauer, J. P.},
doi = {10.1023/A:1019668728098},
file = {:Users/qianxiao/OneDrive - National University of Singapore/Research/Papers/controllability/Balachandran-Dauer2002{\_}Article{\_}ControllabilityOfNonlinearSyst.pdf:pdf},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Exact controllability,K-controllability,approximate controllability,boundary controllability,fixed-point theorems,semigroup theory},
number = {1},
pages = {7--28},
title = {{Controllability of nonlinear systems in Banach spaces: A survey}},
volume = {115},
year = {2002}
}

@article{li2019foundations,
  ids = {liStochasticModifiedEquations2019},
  title = {Stochastic {{Modified Equations}} and {{Dynamics}} of {{Stochastic Gradient Algorithms I}}: {{Mathematical Foundations}}},
  author = {Li, Qianxiao and Tai, Cheng and E, Weinan},
  year = {2019},
  volume = {20},
  pages = {1--47},
  copyright = {All rights reserved},
  file = {/Users/qianxiao/OneDrive - National University of Singapore/Zotero/Li et al_2019_Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I.pdf},
  journal = {Journal of Machine Learning Research},
  number = {40}
}



@inproceedings{Lu2017,
author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
booktitle = {Advances in neural information processing systems},
pages = {6231--6239},
title = {{The expressive power of neural networks: A view from the width}},
year = {2017}
}
@article{crandall1983viscosity,
author = {Crandall, Michael G and Lions, Pierre-Louis},
journal = {Transactions of the American Mathematical Society},
number = {1},
pages = {1--42},
title = {{Viscosity solutions of Hamilton-Jacobi equations}},
volume = {277},
year = {1983}
}
@article{Li2019b,
  ids = {liDeepLearningDynamical2019a},
  title = {Deep {{Learning}} via {{Dynamical Systems}}: {{An Approximation Perspective}}},
  shorttitle = {Deep {{Learning}} via {{Dynamical Systems}}},
  author = {Li, Qianxiao and Lin, Ting and Shen, Zuowei},
  year = {2019},
  abstract = {We build on the dynamical systems approach to deep learning, where deep residual networks are idealized as continuous-time dynamical systems. Although theoretical foundations have been developed on the optimization side through mean-field optimal control theory, the function approximation properties of such models remain largely unexplored, especially when the dynamical systems are controlled by functions of low complexity. In this paper, we establish some basic results on the approximation capabilities of deep learning models in the form of dynamical systems. In particular, we derive general sufficient conditions for universal approximation of functions in Lp using flow maps of dynamical systems, and we also deduce some results on their approximation rates for specific cases. Overall, these results reveal that composition function approximation through flow maps present a new paradigm in approximation theory and contributes to building a useful mathematical framework to investigate deep learning.},
  archivePrefix = {arXiv},
  copyright = {All rights reserved},
  eprint = {1912.10382},
  eprinttype = {arxiv},
  journal = {To appear in J. Eur. Math. Soc.},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  language = {en},
}




@article{aleksandrov1968accumulation,
author = {Aleksandrov, Vladimir V},
journal = {Vestnik MGU},
title = {{On the accumulation of perturbations in the linear systems with two coordinates}},
volume = {3},
year = {1968}
}


@article{dupont2019augmented,
  title={Augmented neural odes},
  author={Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1904.01681},
  year={2019}
}

@article{zhang2019approximation,
  title={Approximation Capabilities of Neural Ordinary Differential Equations},
  author={Zhang, Han and Gao, Xi and Unterman, Jacob and Arodz, Tom},
  journal={arXiv preprint arXiv:1907.12998},
  year={2019}
}


@article{guntherLayerparallelTrainingDeep2020,
  ids = {guntherLayerParallelTrainingDeep2020},
  title = {Layer-Parallel Training of Deep Residual Neural Networks},
  author = {G{\"u}nther, Stefanie and Ruthotto, Lars and Schroder, Jacob B. and Cyr, Eric C. and Gauger, Nicolas R.},
  year = {2020},
  volume = {2},
  pages = {1--23},
  publisher = {{SIAM}},
  file = {C\:\\Users\\li_qi\\Zotero\\storage\\9B5VIX93\\Günther et al. - 2020 - Layer-parallel training of deep residual neural ne.pdf;C\:\\Users\\li_qi\\Zotero\\storage\\V7KMFFIX\\Günther et al. - 2020 - Layer-Parallel Training of Deep Residual Neural Ne.pdf;C\:\\Users\\li_qi\\Zotero\\storage\\CUMC3KRG\\19M1247620.html},
  journal = {SIAM Journal on Mathematics of Data Science},
  number = {1}
}


@inproceedings{li2020onthe,
title={On the Curse of Memory in Recurrent Neural Networks: Approximation and Optimization Analysis},
author={Zhong Li and Jiequn Han and Weinan E and Qianxiao Li},
booktitle={International Conference on Learning Representations (ICLR 2021)},
year={2021},
}

@book{zwanzig2001nonequilibrium,
  title={Nonequilibrium statistical mechanics},
  author={Zwanzig, Robert},
  year={2001},
  publisher={Oxford University Press}
}


% RNN RELATED WORK

% optimization of RNN
@incollection{NIPS2016_6214,
title = {Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations},
author = {Neyshabur, Behnam and Wu, Yuhuai and Salakhutdinov, Russ R and Srebro, Nati},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {3477--3485},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6214-path-normalized-optimization-of-recurrent-neural-networks-with-relu-activations.pdf}
}

@Inbook{Martens2012,
author="Martens, James
and Sutskever, Ilya",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="Training Deep and Recurrent Networks with Hessian-Free Optimization",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="479--535",
abstract="In this chapter we will first describe the basic HF approach, and then examine well-known performance-improving techniques such as preconditioning which we have found to be beneficial for neural network training, as well as others of a more heuristic nature which are harder to justify, but which we have found to work well in practice. We will also provide practical tips for creating efficient and bug-free implementations and discuss various pitfalls which may arise when designing and using an HF-type approach in a particular application.",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_27",
url="https://doi.org/10.1007/978-3-642-35289-8_27"
}

@inproceedings{DBLP:conf/icml/MartensS11,
  author={James Martens and Ilya Sutskever},
  title={Learning Recurrent Neural Networks with Hessian-Free Optimization},
  year={2011},
  cdate={1293840000000},
  pages={1033-1040},
  url={https://icml.cc/2011/papers/532_icmlpaper.pdf},
  booktitle={ICML},
  crossref={conf/icml/2011}
}

@article{Vural2020RNNbasedOL,
  title={RNN-based Online Learning: An Efficient First-Order Optimization Algorithm with a Convergence Guarantee},
  author={N. M. Vural and S. Yilmaz and F. Ilhan and S. Kozat},
  journal={ArXiv},
  year={2020},
  volume={abs/2003.03601}
}

@article{Talathi2015ImprovingPO,
  title={Improving performance of recurrent neural network with relu nonlinearity},
  author={Sachin S. Talathi and Aniket Vartak},
  journal={ArXiv},
  year={2015},
  volume={abs/1511.03771}
} % initializaiton strategy

@article{Li2019ERNNDO,
  title={E-RNN: Design Optimization for Efficient Recurrent Neural Networks in FPGAs},
  author={Z. Li and Caiwen Ding and S. Wang and W. Wen and Youwei Zhuo and Chang Liu and Q. Qiu and W. Xu and X. Lin and Xuehai Qian and Yanzhi Wang},
  journal={2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  year={2019},
  pages={69-80}
}


@article{hardtGradientDescentLearns2019,
  title={Gradient descent learns linear dynamical systems},
  author={Hardt, Moritz and Ma, Tengyu and Recht, Benjamin},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={1025--1068},
  year={2018},
  publisher={JMLR. org}
}
% (linear RNN)

@article{Chitour2018AGA,
  title={A Geometric Approach of Gradient Descent Algorithms in Neural Networks},
  author={Y. Chitour and Zhenyu Liao and R. Couillet},
  journal={ArXiv},
  year={2018},
  volume={abs/1811.03568}
} % (linear NN)

@inproceedings{AllenZhu2019OnTC,
  title={On the Convergence Rate of Training Recurrent Neural Networks},
  author={Zeyuan Allen-Zhu and Y. Li and Z. Song},
  booktitle={NeurIPS},
  year={2019}
}

% Continuous-time RNN
@article{Ceni2019InterpretingRN,
  title={Interpreting Recurrent Neural Networks Behaviour via Excitable Network Attractors},
  author={Andrea Ceni and P. Ashwin and L. Livi},
  journal={Cognitive Computation},
  year={2019},
  volume={12},
  pages={330-356}
}

@inproceedings{
chang2018antisymmetricrnn,
title={Antisymmetric{RNN}: A Dynamical System View on Recurrent Neural Networks},
author={Bo Chang and Minmin Chen and Eldad Haber and Ed H. Chi},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ryxepo0cFX},
}

@article{Lim2020UnderstandingRN,
  title={Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory},
  author={S. H. Lim},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.11052}
}

@article{Sherstinsky2018FundamentalsOR,
  title={Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network},
  author={A. Sherstinsky},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.03314}
}

@article{Niu2019RecurrentNN,
  title={Recurrent Neural Networks in the Eye of Differential Equations},
  author={M. Y. Niu and L. Horesh and I. Chuang},
  journal={ArXiv},
  year={2019},
  volume={abs/1904.12933}
}

@article{Herrera2020TheoreticalGF,
  title={Theoretical Guarantees for Learning Conditional Expectation using Controlled ODE-RNN},
  author={Calypso Herrera and Florian Krach and Josef Teichmann},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.04727}
}

@article{DBLP:journals/corr/abs-1907-03907,
  author    = {Yulia Rubanova and
               Ricky T. Q. Chen and
               David Duvenaud},
  title     = {Latent ODEs for Irregularly-Sampled Time Series},
  journal   = {CoRR},
  volume    = {abs/1907.03907},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.03907},
  archivePrefix = {arXiv},
  eprint    = {1907.03907},
  timestamp = {Wed, 17 Jul 2019 10:27:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-03907.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% Approximation theory for RNNs


@article{schafer2007recurrent,
  title={Recurrent neural networks are universal approximators},
  author={Sch{\"a}fer, Anton Maximilian and Zimmermann, Hans-Georg},
  journal={International journal of neural systems},
  volume={17},
  number={04},
  pages={253--263},
  year={2007},
  publisher={World Scientific}
}

@article{siegelmann1991turing,
  title={Turing computability with neural nets},
  author={Siegelmann, Hava T and Sontag, Eduardo D},
  journal={Applied Mathematics Letters},
  volume={4},
  number={6},
  pages={77--80},
  year={1991},
  publisher={Elsevier}
}


% Approximation theory of operators/functionals
@article{Lu2019DeepONetLN,
  title={DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
  author={Lu Lu and Pengzhan Jin and G. Karniadakis},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03193}
}

@article{chenchen1995universal,
  author={ {Tianping Chen} and  {Hong Chen}},
  journal={IEEE Transactions on Neural Networks},
  title={Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems},
  year={1995},
  volume={6},
  number={4},
  pages={911-917},
  abstract={The purpose of this paper is to investigate neural network capability systematically. The main results are: 1) every Tauber-Wiener function is qualified as an activation function in the hidden layer of a three-layered neural network; 2) for a continuous function in S'(R/sup 1/) to be a Tauber-Wiener function, the necessary and sufficient condition is that it is not a polynomial; 3) the capability of approximating nonlinear functionals defined on some compact set of a Banach space and nonlinear operators has been shown; and 4) the possibility by neural computation to approximate the output as a whole (not at a fixed point) of a dynamical system, thus identifying the system.<>},
  keywords={feedforward neural nets;function approximation;approximation theory;identification;universal approximation;nonlinear operators;arbitrary activation functions;dynamical system identification;nonlinear function approximation;activation function;hidden layer;three-layered neural network;Neural networks;Nonlinear dynamical systems;Computer networks;Kernel;Sufficient conditions;Polynomials;Integral equations;Mathematics;Sun;H infinity control},
  doi={10.1109/72.392253},
  ISSN={1941-0093},
}

% Gradients exploding and vanishing
@InProceedings{pmlr-v107-can20a,
  title = 	 {{Gating creates slow modes and controls phase-space complexity in GRUs and LSTMs}},
  author =       {Can, Tankut and Krishnamurthy, Kamesh and Schwab, David J.},
  pages = 	 {476--511},
  year = 	 {2020},
  volume = 	 {107},
  series = 	 {Mathematical and Scientific Machine Learning},
  publisher =    {PMLR},
  abstract = 	 {Recurrent neural networks (RNNs) are powerful dynamical models for data with complex temporal structure. However, training RNNs has traditionally proved challenging due to exploding or vanishing of gradients. RNN models such as LSTMs and GRUs (and their variants) significantly mitigate these issues associated with training by introducing various types of {\it gating} units into the architecture. While these gates empirically improve performance, how the addition of gates influences the dynamics and trainability of GRUs and LSTMs is not well understood. Here, we take the perspective of studying randomly initialized LSTMs and GRUs as dynamical systems, and ask how the salient dynamical properties are shaped by the gates.  We leverage tools from random matrix theory and mean-field theory to study the state-to-state Jacobians of GRUs and LSTMs. We show that the update gate in the GRU and the forget gate in the LSTM can lead to an accumulation of slow modes in the dynamics. Moreover, the GRU update gate can poise the system at a marginally stable point. The reset gate in the GRU and the output and input gates in the LSTM control the spectral radius of the Jacobian, and the GRU reset gate also modulates the complexity of the landscape of fixed-points. Furthermore, for the GRU we obtain a phase diagram describing the statistical properties of fixed-points. We also provide a preliminary comparison of training performance to the various dynamical regimes realized by varying hyperparameters. Looking to the future, we have introduced a powerful set of techniques which can be adapted to a broad class of RNNs, to study the influence of various architectural choices on dynamics, and potentially motivate the principled discovery of novel architectures.  }
} % (RNN)

@incollection{NIPS2018_7338,
title = {How to Start Training: The Effect of Initialization and Architecture},
author = {Hanin, Boris and Rolnick, David},
booktitle = {Advances in Neural Information Processing Systems 31},
pages = {571--581},
year = {2018},
} % (NN)

@incollection{NIPS2018_7339,
title = {Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients?},
author = {Hanin, Boris},
booktitle = {Advances in Neural Information Processing Systems 31},
pages = {582--591},
year = {2018},
} % (NN)

@article{Li2018IndependentlyRN,
  title={Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN},
  author={S. Li and W. Li and Chris Cook and C. Zhu and Y. Gao},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={5457-5466}
}

% Linear RNN
@article{Martin2018ParallelizingLR,
  title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},
  author={E. Martin and Chris Cundy},
  journal={ArXiv},
  year={2018},
  volume={abs/1709.04057}
}

@article{Chevalier2018LARNNLA,
  title={LARNN: Linear Attention Recurrent Neural Network},
  author={Guillaume Chevalier},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.05578}
}


@Article{CiCP-25-947,
  author={Ma, Chao and Wang, Jianchun and E, Weinan},
  title = {Model Reduction with Memory and the Machine Learning of Dynamical Systems},
    journal = {Communications in Computational Physics},
    year = {2018},
    volume = {25},
    number = {4},
    pages = {947--962},
    issn = {1991-7120},
    doi = {https://doi.org/10.4208/cicp.OA-2018-0269}
}



@article{cuchieroDeepNeuralNetworks2019,
  title = {Deep Neural Networks, Generic Universal Interpolation, and Controlled {{ODEs}}},
  author = {Cuchiero, Christa and Larsson, Martin and Teichmann, Josef},
  year = {2019},
  abstract = {A recent paradigm views deep neural networks as discretizations of certain controlled ordinary differential equations. We make use of this perspective to link expressiveness of deep networks to the notion of controllability of dynamical systems. Using this connection, we study an expressiveness property that we call universal interpolation, and show that it is generic in a certain sense. The universal interpolation property is slightly weaker than universal approximation, and disentangles supervised learning on finite training sets from generalization properties. We also show that universal interpolation holds for certain deep neural networks even if large numbers of parameters are left untrained, and instead chosen randomly. This lends theoretical support to the observation that training with random initialization can be successful even when most parameters are largely unchanged through the training.},
  archivePrefix = {arXiv},
  eprint = {1908.07838},
  eprinttype = {arxiv},
  file = {C\:\\Users\\li_qi\\Zotero\\storage\\XIYQGW8C\\Cuchiero et al. - 2019 - Deep neural networks, generic universal interpolat.pdf},
  journal = {arXiv:1908.07838 [math]},
  keywords = {Mathematics - Dynamical Systems,Mathematics - Optimization and Control},
  language = {en},
}


@article{tabuadaUniversalApproximationPower2020,
  title = {Universal {{Approximation Power}} of {{Deep Residual Neural Networks}} via {{Nonlinear Control Theory}}},
  author = {Tabuada, Paulo and Gharesifard, Bahman},
  year = {2020},
  abstract = {In this paper, we explain the universal approximation capabilities of deep residual neural networks through geometric nonlinear control. Inspired by recent work establishing links between residual networks and control systems, we provide a general sufficient condition for a residual network to have the power of universal approximation by asking the activation function, or one of its derivatives, to satisfy a quadratic differential equation. Many activation functions used in practice satisfy this assumption, exactly or approximately, and we show this property to be sufficient for an adequately deep neural network with 2n states to approximate arbitrarily well, on a compact set and with respect to the supremum norm, any continuous function from Rn to Rn. We further show this result to hold for very simple architectures for which the weights only need to assume two values. The first key technical contribution consists of relating the universal approximation problem to controllability of an ensemble of control systems corresponding to a residual network and to leverage classical Lie algebraic techniques to characterize controllability. The second technical contribution is to identify monotonicity as the bridge between controllability of finite ensembles and uniform approximability on compact sets.},
  archivePrefix = {arXiv},
  eprint = {2007.06007},
  eprinttype = {arxiv},
  journal = {arXiv:2007.06007},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control,Statistics - Machine Learning},
}

@inproceedings{
  chen2021towards,
  title={Towards Robust Neural Networks via Close-loop Control},
  author={Zhuotong Chen and Qianxiao Li and Zheng Zhang},
  booktitle={International Conference on Learning Representations (ICLR 2021)},
  year={2021},
}

@inproceedings{mandt2016variational,
  title={A variational analysis of stochastic gradient algorithms},
  author={Mandt, Stephan and Hoffman, Matthew and Blei, David},
  booktitle={International conference on machine learning},
  pages={354--363},
  year={2016},
  organization={PMLR}
}

@article{warming1974modified,
  title={The modified equation approach to the stability and accuracy analysis of finite-difference methods},
  author={Warming, RF and Hyett, BJ},
  journal={Journal of computational physics},
  volume={14},
  number={2},
  pages={159--179},
  year={1974},
  publisher={Elsevier}
}


@inproceedings{ye2020amata,
  title={Amata: An Annealing Mechanism for Adversarial Training Acceleration},
  author={Ye, Nanyang and Li, Qianxiao and Zhou, Xiao-Yun and Zhu, Zhanxing},
  booktitle={Thirty-Fifth AAAI Conference on Artificial Intelligenc (AAAI 2021)},
  year={2021}
}



@article{jiang2021approximation,
  title={Approximation Theory of Convolutional Architectures for Time Series Modelling},
  author={Jiang, Haotian and Li, Zhong and Li, Qianxiao},
  journal={International Conferences on Machine Learning (ICML 2021)},
  year={2021}
}

@article{onsager1931reciprocal,
  title={Reciprocal relations in irreversible processes. I.},
  author={Onsager, Lars},
  journal={Physical review},
  volume={37},
  number={4},
  pages={405},
  year={1931},
  publisher={APS}
}

@article{onsager1931breciprocal,
  title={Reciprocal relations in irreversible processes. II.},
  author={Onsager, Lars},
  journal={Physical review},
  volume={38},
  number={12},
  pages={2265},
  year={1931},
  publisher={APS}
}

@article{yu2020onsagernet,
  title={OnsagerNet: Learning Stable and Interpretable Dynamics using a Generalized Onsager Principle},
  author={Yu, Haijun and Tian, Xinyuan and E, Weinan and Li, Qianxiao},
  journal={arXiv preprint arXiv:2009.02327},
  year={2020}
}

@article{li2019computing,
  title={Computing committor functions for the study of rare events using deep learning},
  author={Li, Qianxiao and Lin, Bo and Ren, Weiqing},
  journal={The Journal of Chemical Physics},
  volume={151},
  number={5},
  pages={054112},
  year={2019},
  publisher={AIP Publishing LLC}
}

@inproceedings{lin2020data,
  title={A Data Driven Method for Computing Quasipotentials},
  author={Lin, Bo and Li, Qianxiao and Ren, Weiqing},
  booktitle={Mathematical and Scientific Machine Learning (MSML 2021)},
  url={[arXiv preprint arXiv:2012.09111]},
  year={2021}
}

@inproceedings{cai2019quantitative,
  title={A quantitative analysis of the effect of batch normalization on gradient descent},
  author={Cai, Yongqiang and Li, Qianxiao and Shen, Zuowei},
  booktitle={International Conference on Machine Learning (ICML 2019)},
  pages={882--890},
  year={2019},
  organization={PMLR}
}


@book{freidlinRandomPerturbationsDynamical2012,
  title = {Random {{Perturbations}} of {{Dynamical Systems}}},
  author = {Freidlin, Mark I. and Wentzell, Alexander D.},
  year = {2012},
  series = {Grundlehren Der Mathematischen {{Wissenschaften}}},
  volume = {260},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-25847-3},
  isbn = {978-3-642-25846-6 978-3-642-25847-3},
  language = {en},
}

@inproceedings{jiang2021approx,
	title = {Approximation {Theory} of {Convolutional} {Architectures} for {Time} {Series} {Modelling}},
	abstract = {We study the approximation properties of convolutional architectures applied to time series modelling, which can be formulated mathematically as a functional approximation problem. In the recurrent setting, recent results reveal an intricate connection between approximation efficiency and memory structures in the data generation process. In this paper, we derive parallel results for convolutional architectures, with WaveNet being a prime example. Our results reveal that in this new setting, approximation efficiency is not only characterised by memory, but also additional fine structures in the target relationship. This leads to a novel definition of spectrum-based regularity that measures the complexity of temporal relationships under the convolutional approximation scheme. These analyses provide a foundation to understand the differences between architectural choices for time series modelling and can give theoretically grounded guidance for practical applications.},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning} (ICML 2021)},
	publisher = {PMLR},
	author = {Jiang, Haotian and Li, Zhong and Li, Qianxiao},
	month = jul,
	year = {2021},
	keywords = {Convolutional, Theoritical},
	pages = {4961--4970},
}

@inproceedings{li2021approxed,
	title = {On the approximation properties of recurrent encoder-decoder architectures},
	abstract = {Encoder-decoder architectures have recently gained popularity in sequence to sequence modelling, featuring in state-of-the-art models such as transformers. However, a mathematical understanding of...},
	booktitle={International Conference on Learning Representations (ICLR 2022)},
	author = {Li, Zhong and Jiang, Haotian and Li, Qianxiao},
	month = sep,
	year = {2022},
}

@article{li2022approxlinear,
	title = {Approximation and {Optimization} {Theory} for {Linear} {Continuous}-{Time} {Recurrent} {Neural} {Networks}},
	volume = {23},
	number = {42},
	journal = {Journal of Machine Learning Research},
	author = {Li, Zhong and Han, Jiequn and E, Weinan and Li, Qianxiao},
	year = {2022},
	pages = {1--85},
}

@article{oord2016wave,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2022-02-09},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03499},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
}

@article{li2022deep,
  title={Deep learning via dynamical systems: An approximation perspective},
  author={Li, Qianxiao and Lin, Ting and Shen, Zuowei},
  journal={Journal of the European Mathematical Society},
  year={2022}
}
